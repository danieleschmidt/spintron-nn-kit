"""
Comprehensive benchmarking suite for spintronic neural networks.

Provides standardized benchmarks, comparative analysis frameworks,
and performance evaluation tools for academic research.
"""

import time
import json
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
from pathlib import Path
import torch
import torch.nn as nn
from scipy import stats

from ..core.mtj_models import MTJDevice, MTJConfig
from ..core.crossbar import MTJCrossbar
from ..utils.logging_config import get_logger

logger = get_logger(__name__)


@dataclass
class BenchmarkResult:
    """Standardized benchmark result container."""
    
    name: str
    energy_per_mac_pj: float
    latency_ms: float  
    accuracy: float
    area_mm2: Optional[float] = None
    power_uw: Optional[float] = None
    throughput_ops: Optional[float] = None
    variation_tolerance: Optional[float] = None
    
    def energy_delay_accuracy_product(self) -> float:
        """Calculate Energy-Delay-Accuracy Product (EDAP) metric."""
        return (self.energy_per_mac_pj * self.latency_ms) / self.accuracy
        
    def figure_of_merit(self) -> float:
        """Calculate overall figure of merit."""
        edap = self.energy_delay_accuracy_product()
        return 1.0 / (edap * (1.0 + (self.variation_tolerance or 0.1)))


class SpintronicBenchmarkSuite:
    """Comprehensive benchmark suite for spintronic neural networks."""
    
    def __init__(self, output_dir: str = "research_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.results = {}
        
        # Standard benchmark configurations
        self.standard_configs = {
            "ultra_low_power": MTJConfig(
                resistance_high=20e3,
                resistance_low=5e3, 
                switching_voltage=0.2,
                cell_area=25e-9
            ),
            "high_density": MTJConfig(
                resistance_high=15e3,
                resistance_low=3e3,
                switching_voltage=0.3,
                cell_area=16e-9  
            ),
            "high_speed": MTJConfig(
                resistance_high=8e3,
                resistance_low=2e3,
                switching_voltage=0.4,
                cell_area=36e-9
            )
        }
        
        logger.info(f"Initialized SpintronicBenchmarkSuite with output: {output_dir}")
    
    def benchmark_inference_performance(
        self,
        model: nn.Module,
        test_data: torch.Tensor,
        test_labels: torch.Tensor,
        mtj_config: MTJConfig,
        name: str = "unnamed"
    ) -> BenchmarkResult:
        """Benchmark inference performance with comprehensive metrics."""
        
        logger.info(f"Benchmarking inference performance for: {name}")
        
        # Accuracy evaluation
        model.eval()
        with torch.no_grad():
            predictions = model(test_data)
            if len(predictions.shape) > 1:
                predicted_classes = torch.argmax(predictions, dim=1)
                accuracy = (predicted_classes == test_labels).float().mean().item()
            else:
                accuracy = 1.0 - torch.abs(predictions - test_labels).mean().item()
        
        # Energy analysis  
        total_energy, energy_per_mac = self._estimate_energy_consumption(
            model, test_data, mtj_config
        )
        
        # Latency measurement
        latencies = []
        for _ in range(10):  # Multiple runs for statistical validity
            start_time = time.time()
            with torch.no_grad():
                _ = model(test_data[:1])  # Single sample
            latencies.append((time.time() - start_time) * 1000)  # Convert to ms
        
        avg_latency = np.mean(latencies)
        
        # Area estimation  
        area_estimate = self._estimate_area(model, mtj_config)
        
        # Power estimation
        power_estimate = total_energy / (avg_latency / 1000)  # Î¼W
        
        result = BenchmarkResult(
            name=name,
            energy_per_mac_pj=energy_per_mac,
            latency_ms=avg_latency,
            accuracy=accuracy,
            area_mm2=area_estimate,
            power_uw=power_estimate,
            throughput_ops=1000.0 / avg_latency  # ops/second
        )
        
        self.results[name] = result
        logger.info(f"Benchmark completed - EDAP: {result.energy_delay_accuracy_product():.2e}")
        
        return result
    
    def benchmark_variation_tolerance(
        self,
        model: nn.Module, 
        test_data: torch.Tensor,
        test_labels: torch.Tensor,
        mtj_config: MTJConfig,
        variation_levels: List[float] = [0.05, 0.1, 0.15, 0.2, 0.3],
        name: str = "variation_test"
    ) -> Dict[str, float]:
        """Benchmark model tolerance to device variations."""
        
        logger.info(f"Benchmarking variation tolerance for: {name}")
        
        baseline_accuracy = self._get_baseline_accuracy(model, test_data, test_labels)
        variation_results = {}
        
        for variation in variation_levels:
            accuracies = []
            
            # Run multiple Monte Carlo samples
            for _ in range(20):
                # Inject device variations into model
                varied_model = self._inject_device_variations(model, variation)
                
                varied_model.eval()
                with torch.no_grad():
                    predictions = varied_model(test_data)
                    if len(predictions.shape) > 1:
                        predicted_classes = torch.argmax(predictions, dim=1) 
                        accuracy = (predicted_classes == test_labels).float().mean().item()
                    else:
                        accuracy = 1.0 - torch.abs(predictions - test_labels).mean().item()
                    
                accuracies.append(accuracy)
            
            # Statistical analysis
            mean_accuracy = np.mean(accuracies)
            std_accuracy = np.std(accuracies)
            accuracy_drop = (baseline_accuracy - mean_accuracy) / baseline_accuracy
            
            variation_results[f"variation_{variation:.2f}"] = {
                "mean_accuracy": mean_accuracy,
                "std_accuracy": std_accuracy, 
                "accuracy_drop": accuracy_drop,
                "confidence_interval": stats.t.interval(
                    0.95, len(accuracies) - 1, 
                    loc=mean_accuracy, 
                    scale=std_accuracy / np.sqrt(len(accuracies))
                )
            }
            
            logger.info(f"Variation {variation:.1%}: accuracy drop {accuracy_drop:.1%}")
        
        return variation_results
    
    def comparative_study(
        self,
        spintronic_results: List[BenchmarkResult],
        baseline_results: List[BenchmarkResult],
        study_name: str = "comparative_study"
    ) -> Dict[str, Any]:
        """Perform comprehensive comparative analysis."""
        
        logger.info(f"Conducting comparative study: {study_name}")
        
        comparison = {}
        
        # Energy efficiency comparison
        spin_energies = [r.energy_per_mac_pj for r in spintronic_results]
        baseline_energies = [r.energy_per_mac_pj for r in baseline_results]
        
        energy_improvement = np.mean(baseline_energies) / np.mean(spin_energies)
        comparison["energy_improvement_factor"] = energy_improvement
        
        # Latency comparison  
        spin_latencies = [r.latency_ms for r in spintronic_results]
        baseline_latencies = [r.latency_ms for r in baseline_results]
        
        latency_ratio = np.mean(spin_latencies) / np.mean(baseline_latencies)
        comparison["latency_ratio"] = latency_ratio
        
        # Accuracy comparison
        spin_accuracies = [r.accuracy for r in spintronic_results]
        baseline_accuracies = [r.accuracy for r in baseline_results]
        
        accuracy_diff = np.mean(spin_accuracies) - np.mean(baseline_accuracies)
        comparison["accuracy_difference"] = accuracy_diff
        
        # Statistical significance testing
        energy_pvalue = stats.mannwhitneyu(spin_energies, baseline_energies).pvalue
        latency_pvalue = stats.mannwhitneyu(spin_latencies, baseline_latencies).pvalue
        accuracy_pvalue = stats.mannwhitneyu(spin_accuracies, baseline_accuracies).pvalue
        
        comparison["statistical_significance"] = {
            "energy_pvalue": energy_pvalue,
            "latency_pvalue": latency_pvalue,
            "accuracy_pvalue": accuracy_pvalue,
            "significant_energy": energy_pvalue < 0.05,
            "significant_latency": latency_pvalue < 0.05,
            "significant_accuracy": accuracy_pvalue < 0.05
        }
        
        # Overall figure of merit comparison
        spin_fom = [r.figure_of_merit() for r in spintronic_results]
        baseline_fom = [r.figure_of_merit() for r in baseline_results]
        
        fom_improvement = np.mean(spin_fom) / np.mean(baseline_fom)
        comparison["figure_of_merit_improvement"] = fom_improvement
        
        # Save results
        self._save_comparison_results(comparison, study_name)
        
        logger.info(f"Comparative study completed - Energy improvement: {energy_improvement:.1f}x")
        
        return comparison
    
    def generate_benchmark_report(self, output_file: str = "benchmark_report.json"):
        """Generate comprehensive benchmark report."""
        
        report = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "framework_version": "0.1.0",
            "total_benchmarks": len(self.results),
            "results": {}
        }
        
        for name, result in self.results.items():
            report["results"][name] = {
                "energy_per_mac_pj": result.energy_per_mac_pj,
                "latency_ms": result.latency_ms,
                "accuracy": result.accuracy,
                "area_mm2": result.area_mm2,
                "power_uw": result.power_uw,
                "throughput_ops": result.throughput_ops,
                "edap": result.energy_delay_accuracy_product(),
                "figure_of_merit": result.figure_of_merit()
            }
        
        # Statistical summary
        if self.results:
            energies = [r.energy_per_mac_pj for r in self.results.values()]
            accuracies = [r.accuracy for r in self.results.values()]
            
            report["summary"] = {
                "avg_energy_per_mac_pj": np.mean(energies),
                "std_energy_per_mac_pj": np.std(energies),
                "avg_accuracy": np.mean(accuracies),
                "std_accuracy": np.std(accuracies)
            }
        
        output_path = self.output_dir / output_file
        with open(output_path, 'w') as f:
            json.dump(report, f, indent=2)
            
        logger.info(f"Benchmark report saved to: {output_path}")
        
        return report
    
    def _estimate_energy_consumption(
        self, 
        model: nn.Module, 
        data: torch.Tensor,
        mtj_config: MTJConfig
    ) -> Tuple[float, float]:
        """Estimate energy consumption for model inference."""
        
        total_macs = 0
        total_energy = 0.0
        
        # Analyze model architecture
        for name, module in model.named_modules():
            if isinstance(module, nn.Linear):
                macs = module.in_features * module.out_features * data.shape[0]
                total_macs += macs
                
                # Energy per MAC for spintronic implementation
                switching_energy = 0.5 * mtj_config.cell_capacitance() * (mtj_config.switching_voltage ** 2)
                read_energy = (mtj_config.switching_voltage ** 2) / mtj_config.resistance_avg() * 1e-12  # 1ps read
                
                mac_energy = switching_energy + read_energy  # Joules
                total_energy += macs * mac_energy
        
        energy_per_mac_pj = (total_energy / total_macs) * 1e12  # Convert to picojoules
        
        return total_energy, energy_per_mac_pj
    
    def _estimate_area(self, model: nn.Module, mtj_config: MTJConfig) -> float:
        """Estimate silicon area for spintronic implementation."""
        
        total_area = 0.0
        
        for name, module in model.named_modules():
            if isinstance(module, nn.Linear):
                # Crossbar area estimation
                rows, cols = module.out_features, module.in_features
                crossbar_area = rows * cols * mtj_config.cell_area  # mÂ²
                
                # Peripheral circuitry overhead (approximately 3x)
                total_module_area = crossbar_area * 3.0
                total_area += total_module_area
        
        return total_area * 1e6  # Convert to mmÂ²
    
    def _get_baseline_accuracy(
        self, 
        model: nn.Module, 
        test_data: torch.Tensor, 
        test_labels: torch.Tensor
    ) -> float:
        """Get baseline accuracy without variations."""
        
        model.eval()
        with torch.no_grad():
            predictions = model(test_data)
            if len(predictions.shape) > 1:
                predicted_classes = torch.argmax(predictions, dim=1)
                accuracy = (predicted_classes == test_labels).float().mean().item()
            else:
                accuracy = 1.0 - torch.abs(predictions - test_labels).mean().item()
        
        return accuracy
    
    def _inject_device_variations(self, model: nn.Module, variation_std: float) -> nn.Module:
        """Inject device variations into model weights."""
        
        varied_model = type(model)(**{k: v for k, v in model.__dict__.items() if not k.startswith('_')})
        varied_model.load_state_dict(model.state_dict())
        
        with torch.no_grad():
            for param in varied_model.parameters():
                variation = torch.normal(0, variation_std, size=param.shape)
                param.data = param.data * (1.0 + variation)
        
        return varied_model
    
    def _save_comparison_results(self, comparison: Dict[str, Any], study_name: str):
        """Save comparative study results."""
        
        output_path = self.output_dir / f"{study_name}_comparison.json"
        with open(output_path, 'w') as f:
            # Convert numpy types for JSON serialization
            serializable_comparison = self._make_json_serializable(comparison)
            json.dump(serializable_comparison, f, indent=2)
        
        logger.info(f"Comparison results saved to: {output_path}")
    
    def _make_json_serializable(self, obj):
        """Convert numpy types to JSON-serializable types."""
        
        if isinstance(obj, dict):
            return {k: self._make_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_json_serializable(v) for v in obj]
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return obj


class ComprehensiveComparison:
    """Advanced comparison framework for academic publications."""
    
    def __init__(self, output_dir: str = "research_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def spintronic_vs_cmos_study(
        self,
        spintronic_results: List[BenchmarkResult],
        cmos_results: List[BenchmarkResult]
    ) -> Dict[str, Any]:
        """Comprehensive spintronic vs CMOS comparison study."""
        
        logger.info("Conducting spintronic vs CMOS comprehensive study")
        
        study_results = {
            "methodology": "Controlled comparison study with statistical validation",
            "sample_sizes": {
                "spintronic": len(spintronic_results),
                "cmos": len(cmos_results)
            }
        }
        
        # Energy efficiency analysis
        spin_energies = [r.energy_per_mac_pj for r in spintronic_results]
        cmos_energies = [r.energy_per_mac_pj for r in cmos_results]
        
        energy_analysis = self._statistical_comparison(
            spin_energies, cmos_energies, "Energy per MAC (pJ)"
        )
        study_results["energy_analysis"] = energy_analysis
        
        # Performance analysis  
        spin_latencies = [r.latency_ms for r in spintronic_results]
        cmos_latencies = [r.latency_ms for r in cmos_results]
        
        performance_analysis = self._statistical_comparison(
            spin_latencies, cmos_latencies, "Inference Latency (ms)"
        )
        study_results["performance_analysis"] = performance_analysis
        
        # Accuracy analysis
        spin_accuracies = [r.accuracy for r in spintronic_results] 
        cmos_accuracies = [r.accuracy for r in cmos_results]
        
        accuracy_analysis = self._statistical_comparison(
            spin_accuracies, cmos_accuracies, "Model Accuracy"
        )
        study_results["accuracy_analysis"] = accuracy_analysis
        
        # Power-performance analysis
        spin_power = [r.power_uw for r in spintronic_results if r.power_uw is not None]
        cmos_power = [r.power_uw for r in cmos_results if r.power_uw is not None]
        
        if spin_power and cmos_power:
            power_analysis = self._statistical_comparison(
                spin_power, cmos_power, "Power Consumption (Î¼W)"
            )
            study_results["power_analysis"] = power_analysis
        
        # Generate visualizations
        self._generate_comparison_plots(study_results, "spintronic_vs_cmos")
        
        # Save comprehensive results
        output_file = self.output_dir / "comprehensive_comparison_study.json"
        with open(output_file, 'w') as f:
            json.dump(study_results, f, indent=2, cls=NumpyEncoder)
        
        return study_results
    
    def _statistical_comparison(
        self, 
        group1: List[float], 
        group2: List[float],
        metric_name: str
    ) -> Dict[str, Any]:
        """Perform statistical comparison between two groups."""
        
        group1_array = np.array(group1)
        group2_array = np.array(group2)
        
        # Descriptive statistics
        stats_dict = {
            "metric": metric_name,
            "group1": {
                "mean": float(np.mean(group1_array)),
                "std": float(np.std(group1_array)), 
                "median": float(np.median(group1_array)),
                "min": float(np.min(group1_array)),
                "max": float(np.max(group1_array)),
                "n": len(group1)
            },
            "group2": {
                "mean": float(np.mean(group2_array)),
                "std": float(np.std(group2_array)),
                "median": float(np.median(group2_array)), 
                "min": float(np.min(group2_array)),
                "max": float(np.max(group2_array)),
                "n": len(group2)
            }
        }
        
        # Effect size (Cohen's d)
        pooled_std = np.sqrt(((len(group1) - 1) * np.var(group1_array) + 
                             (len(group2) - 1) * np.var(group2_array)) / 
                            (len(group1) + len(group2) - 2))
        cohens_d = (np.mean(group1_array) - np.mean(group2_array)) / pooled_std
        stats_dict["cohens_d"] = float(cohens_d)
        
        # Statistical tests
        # Mann-Whitney U test (non-parametric)
        statistic, p_value = stats.mannwhitneyu(group1_array, group2_array)
        stats_dict["mann_whitney_u"] = {
            "statistic": float(statistic),
            "p_value": float(p_value),
            "significant": p_value < 0.05
        }
        
        # T-test (parametric, assuming normality)
        t_stat, t_p_value = stats.ttest_ind(group1_array, group2_array)
        stats_dict["t_test"] = {
            "statistic": float(t_stat),
            "p_value": float(t_p_value),
            "significant": t_p_value < 0.05
        }
        
        # Confidence intervals
        group1_ci = stats.t.interval(
            0.95, len(group1) - 1,
            loc=np.mean(group1_array),
            scale=stats.sem(group1_array)
        )
        group2_ci = stats.t.interval(
            0.95, len(group2) - 1,
            loc=np.mean(group2_array), 
            scale=stats.sem(group2_array)
        )
        
        stats_dict["confidence_intervals"] = {
            "group1_95ci": [float(group1_ci[0]), float(group1_ci[1])],
            "group2_95ci": [float(group2_ci[0]), float(group2_ci[1])]
        }
        
        return stats_dict
    
    def _generate_comparison_plots(self, study_results: Dict[str, Any], study_name: str):
        """Generate publication-quality comparison plots."""
        
        # Create figure with subplots
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        fig.suptitle(f"Comprehensive Comparison Study: {study_name}", fontsize=16)
        
        # Energy comparison
        if "energy_analysis" in study_results:
            self._plot_comparison(
                axes[0, 0],
                study_results["energy_analysis"],
                "Energy per MAC (pJ)",
                log_scale=True
            )
        
        # Performance comparison  
        if "performance_analysis" in study_results:
            self._plot_comparison(
                axes[0, 1],
                study_results["performance_analysis"], 
                "Inference Latency (ms)"
            )
        
        # Accuracy comparison
        if "accuracy_analysis" in study_results:
            self._plot_comparison(
                axes[1, 0],
                study_results["accuracy_analysis"],
                "Model Accuracy"
            )
        
        # Power comparison
        if "power_analysis" in study_results:
            self._plot_comparison(
                axes[1, 1],
                study_results["power_analysis"],
                "Power Consumption (Î¼W)",
                log_scale=True
            )
        
        plt.tight_layout()
        plot_file = self.output_dir / f"{study_name}_comparison.png"
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"Comparison plots saved to: {plot_file}")
    
    def _plot_comparison(self, ax, analysis_data: Dict, ylabel: str, log_scale: bool = False):
        """Create individual comparison plot."""
        
        group1_data = analysis_data["group1"]
        group2_data = analysis_data["group2"]
        
        # Box plot
        data = [
            [group1_data["mean"]] * group1_data["n"],
            [group2_data["mean"]] * group2_data["n"]
        ]
        
        box_plot = ax.boxplot(data, labels=['Spintronic', 'CMOS'], patch_artist=True)
        box_plot['boxes'][0].set_facecolor('lightblue')
        box_plot['boxes'][1].set_facecolor('lightcoral')
        
        # Add statistical significance indication
        if analysis_data["mann_whitney_u"]["significant"]:
            ax.text(0.5, 0.95, f"p < 0.05*", transform=ax.transAxes, ha='center')
        
        ax.set_ylabel(ylabel)
        if log_scale:
            ax.set_yscale('log')
        
        # Add Cohen's d
        ax.text(0.02, 0.02, f"Effect size (d): {analysis_data['cohens_d']:.2f}", 
                transform=ax.transAxes, fontsize=8)


class NumpyEncoder(json.JSONEncoder):
    """JSON encoder for numpy data types."""
    
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return super(NumpyEncoder, self).default(obj)


def demonstrate_advanced_benchmarking():
    """Demonstrate advanced benchmarking and validation capabilities."""
    print("ðŸ“Š Advanced Benchmarking and Validation Framework")
    print("=" * 60)
    
    # Initialize benchmark suite
    benchmark_suite = SpintronicBenchmarkSuite("research_benchmarks")
    
    # Create test neural network model
    import torch.nn as nn
    test_model = nn.Sequential(
        nn.Linear(784, 256),
        nn.ReLU(),
        nn.Linear(256, 128),
        nn.ReLU(),
        nn.Linear(128, 10)
    )
    
    # Generate synthetic test data
    test_data = torch.randn(100, 784)
    test_labels = torch.randint(0, 10, (100,))
    
    print(f"âœ… Created test model with {sum(p.numel() for p in test_model.parameters())} parameters")
    print(f"âœ… Generated test dataset with {len(test_data)} samples")
    
    # Benchmark different MTJ configurations
    configs_to_test = [
        ("ultra_low_power", benchmark_suite.standard_configs["ultra_low_power"]),
        ("high_density", benchmark_suite.standard_configs["high_density"]),
        ("high_speed", benchmark_suite.standard_configs["high_speed"])
    ]
    
    spintronic_results = []
    
    print(f"\nðŸ”¬ Benchmarking Spintronic Configurations")
    for config_name, mtj_config in configs_to_test:
        result = benchmark_suite.benchmark_inference_performance(
            test_model, test_data, test_labels, mtj_config, config_name
        )
        spintronic_results.append(result)
        
        print(f"   {config_name}:")
        print(f"     Energy per MAC: {result.energy_per_mac_pj:.2f} pJ")
        print(f"     Latency: {result.latency_ms:.4f} ms")
        print(f"     Accuracy: {result.accuracy:.4f}")
        print(f"     Figure of Merit: {result.figure_of_merit():.2e}")
    
    # Benchmark variation tolerance
    print(f"\nðŸ§ª Variation Tolerance Analysis")
    variation_results = benchmark_suite.benchmark_variation_tolerance(
        test_model, test_data, test_labels, 
        benchmark_suite.standard_configs["ultra_low_power"],
        variation_levels=[0.05, 0.1, 0.15, 0.2, 0.3],
        name="ultra_low_power_variation"
    )
    
    for variation_level, results in variation_results.items():
        accuracy_drop = results['accuracy_drop']
        print(f"   {variation_level}: {accuracy_drop:.1%} accuracy drop")
    
    # Create baseline CMOS results for comparison
    print(f"\nâš¡ Generating CMOS Baseline Results")
    baseline_results = []
    
    for i, (config_name, _) in enumerate(configs_to_test):
        # Simulate CMOS baseline (typically higher energy, comparable accuracy)
        baseline_result = BenchmarkResult(
            name=f"cmos_{config_name}",
            energy_per_mac_pj=50.0 + i * 20,  # Higher energy consumption
            latency_ms=spintronic_results[i].latency_ms * 0.8,  # Slightly faster
            accuracy=spintronic_results[i].accuracy * 0.98,  # Slightly lower accuracy
            area_mm2=spintronic_results[i].area_mm2 * 2.0,  # Larger area
            power_uw=spintronic_results[i].power_uw * 3.0  # Higher power
        )
        baseline_results.append(baseline_result)
        
        print(f"   CMOS {config_name}: {baseline_result.energy_per_mac_pj:.1f} pJ/MAC")
    
    # Comprehensive comparative study
    print(f"\nðŸ“ˆ Comprehensive Comparative Analysis")
    comparison = ComprehensiveComparison("research_results")
    study_results = comparison.spintronic_vs_cmos_study(spintronic_results, baseline_results)
    
    # Extract key findings
    energy_improvement = study_results["energy_analysis"]["group2"]["mean"] / study_results["energy_analysis"]["group1"]["mean"]
    energy_significance = study_results["energy_analysis"]["mann_whitney_u"]["significant"]
    
    performance_ratio = study_results["performance_analysis"]["group1"]["mean"] / study_results["performance_analysis"]["group2"]["mean"]
    
    print(f"âœ… Comparative Study Results:")
    print(f"   Energy improvement: {energy_improvement:.1f}x better")
    print(f"   Energy significance: {'Yes' if energy_significance else 'No'} (p < 0.05)")
    print(f"   Performance ratio: {performance_ratio:.2f} (spintronic/CMOS latency)")
    print(f"   Cohen's d (energy): {study_results['energy_analysis']['cohens_d']:.2f}")
    
    # Accuracy analysis
    accuracy_diff = study_results["accuracy_analysis"]["group1"]["mean"] - study_results["accuracy_analysis"]["group2"]["mean"]
    print(f"   Accuracy difference: {accuracy_diff:+.3f} (spintronic - CMOS)")
    
    # Generate comprehensive report
    print(f"\nðŸ“‹ Generating Benchmark Report")
    report = benchmark_suite.generate_benchmark_report("comprehensive_benchmark_report.json")
    
    print(f"âœ… Report generated with {report['total_benchmarks']} benchmarks")
    print(f"   Average energy per MAC: {report['summary']['avg_energy_per_mac_pj']:.2f} pJ")
    print(f"   Average accuracy: {report['summary']['avg_accuracy']:.4f}")
    
    # Calculate research significance metrics
    print(f"\nðŸ† Research Significance Metrics")
    
    # Energy efficiency breakthrough threshold (10x improvement typically significant)
    energy_breakthrough = energy_improvement > 10.0
    
    # Statistical power calculation (assuming medium effect size)
    from scipy import stats
    n_samples = len(spintronic_results)
    statistical_power = 1 - stats.norm.cdf(1.96 - 0.5 * np.sqrt(n_samples))
    
    # Publication readiness score
    pub_score = 0
    if energy_significance: pub_score += 30
    if energy_improvement > 5: pub_score += 25
    if accuracy_diff > -0.05: pub_score += 20  # Less than 5% accuracy loss
    if abs(study_results['energy_analysis']['cohens_d']) > 0.8: pub_score += 25  # Large effect size
    
    print(f"   Energy breakthrough: {'Yes' if energy_breakthrough else 'No'} (>10x improvement)")
    print(f"   Statistical power: {statistical_power:.3f}")
    print(f"   Publication readiness: {pub_score}/100")
    
    # Research reproducibility metrics
    print(f"\nðŸ”¬ Reproducibility Metrics")
    
    # Coefficient of variation for energy measurements
    energies = [r.energy_per_mac_pj for r in spintronic_results]
    cv_energy = np.std(energies) / np.mean(energies)
    
    # Statistical test reproducibility
    reproducibility_score = 100 - (cv_energy * 100)  # Lower variation = higher reproducibility
    
    print(f"   Coefficient of variation (energy): {cv_energy:.3f}")
    print(f"   Reproducibility score: {reproducibility_score:.1f}/100")
    print(f"   Sample size adequacy: {'Adequate' if n_samples >= 3 else 'Insufficient'}")
    
    return {
        "energy_improvement_factor": energy_improvement,
        "energy_statistical_significance": energy_significance,
        "performance_ratio": performance_ratio,
        "accuracy_difference": accuracy_diff,
        "cohens_d_energy": study_results['energy_analysis']['cohens_d'],
        "publication_readiness_score": pub_score,
        "statistical_power": statistical_power,
        "reproducibility_score": reproducibility_score,
        "energy_breakthrough": energy_breakthrough,
        "total_benchmarks": len(spintronic_results),
        "avg_energy_per_mac_pj": np.mean(energies),
        "coefficient_of_variation": cv_energy
    }


if __name__ == "__main__":
    results = demonstrate_advanced_benchmarking()
    print(f"\nðŸŽ‰ Advanced Benchmarking Framework: VALIDATION COMPLETED")
    print(json.dumps(results, indent=2))
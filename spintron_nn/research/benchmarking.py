"""
Comprehensive benchmarking suite for spintronic neural networks.

Provides standardized benchmarks, comparative analysis frameworks,
and performance evaluation tools for academic research.
"""

import time
import json
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
from pathlib import Path
import torch
import torch.nn as nn
from scipy import stats

from ..core.mtj_models import MTJDevice, MTJConfig
from ..core.crossbar import MTJCrossbar
from ..utils.logging_config import get_logger

logger = get_logger(__name__)


@dataclass
class BenchmarkResult:
    """Standardized benchmark result container."""
    
    name: str
    energy_per_mac_pj: float
    latency_ms: float  
    accuracy: float
    area_mm2: Optional[float] = None
    power_uw: Optional[float] = None
    throughput_ops: Optional[float] = None
    variation_tolerance: Optional[float] = None
    
    def energy_delay_accuracy_product(self) -> float:
        """Calculate Energy-Delay-Accuracy Product (EDAP) metric."""
        return (self.energy_per_mac_pj * self.latency_ms) / self.accuracy
        
    def figure_of_merit(self) -> float:
        """Calculate overall figure of merit."""
        edap = self.energy_delay_accuracy_product()
        return 1.0 / (edap * (1.0 + (self.variation_tolerance or 0.1)))


class SpintronicBenchmarkSuite:
    """Comprehensive benchmark suite for spintronic neural networks."""
    
    def __init__(self, output_dir: str = "research_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.results = {}
        
        # Standard benchmark configurations
        self.standard_configs = {
            "ultra_low_power": MTJConfig(
                resistance_high=20e3,
                resistance_low=5e3, 
                switching_voltage=0.2,
                cell_area=25e-9
            ),
            "high_density": MTJConfig(
                resistance_high=15e3,
                resistance_low=3e3,
                switching_voltage=0.3,
                cell_area=16e-9  
            ),
            "high_speed": MTJConfig(
                resistance_high=8e3,
                resistance_low=2e3,
                switching_voltage=0.4,
                cell_area=36e-9
            )
        }
        
        logger.info(f"Initialized SpintronicBenchmarkSuite with output: {output_dir}")
    
    def benchmark_inference_performance(
        self,
        model: nn.Module,
        test_data: torch.Tensor,
        test_labels: torch.Tensor,
        mtj_config: MTJConfig,
        name: str = "unnamed"
    ) -> BenchmarkResult:
        """Benchmark inference performance with comprehensive metrics."""
        
        logger.info(f"Benchmarking inference performance for: {name}")
        
        # Accuracy evaluation
        model.eval()
        with torch.no_grad():
            predictions = model(test_data)
            if len(predictions.shape) > 1:
                predicted_classes = torch.argmax(predictions, dim=1)
                accuracy = (predicted_classes == test_labels).float().mean().item()
            else:
                accuracy = 1.0 - torch.abs(predictions - test_labels).mean().item()
        
        # Energy analysis  
        total_energy, energy_per_mac = self._estimate_energy_consumption(
            model, test_data, mtj_config
        )
        
        # Latency measurement
        latencies = []
        for _ in range(10):  # Multiple runs for statistical validity
            start_time = time.time()
            with torch.no_grad():
                _ = model(test_data[:1])  # Single sample
            latencies.append((time.time() - start_time) * 1000)  # Convert to ms
        
        avg_latency = np.mean(latencies)
        
        # Area estimation  
        area_estimate = self._estimate_area(model, mtj_config)
        
        # Power estimation
        power_estimate = total_energy / (avg_latency / 1000)  # Î¼W
        
        result = BenchmarkResult(
            name=name,
            energy_per_mac_pj=energy_per_mac,
            latency_ms=avg_latency,
            accuracy=accuracy,
            area_mm2=area_estimate,
            power_uw=power_estimate,
            throughput_ops=1000.0 / avg_latency  # ops/second
        )
        
        self.results[name] = result
        logger.info(f"Benchmark completed - EDAP: {result.energy_delay_accuracy_product():.2e}")
        
        return result
    
    def benchmark_variation_tolerance(
        self,
        model: nn.Module, 
        test_data: torch.Tensor,
        test_labels: torch.Tensor,
        mtj_config: MTJConfig,
        variation_levels: List[float] = [0.05, 0.1, 0.15, 0.2, 0.3],
        name: str = "variation_test"
    ) -> Dict[str, float]:
        """Benchmark model tolerance to device variations."""
        
        logger.info(f"Benchmarking variation tolerance for: {name}")
        
        baseline_accuracy = self._get_baseline_accuracy(model, test_data, test_labels)
        variation_results = {}
        
        for variation in variation_levels:
            accuracies = []
            
            # Run multiple Monte Carlo samples
            for _ in range(20):
                # Inject device variations into model
                varied_model = self._inject_device_variations(model, variation)
                
                varied_model.eval()
                with torch.no_grad():
                    predictions = varied_model(test_data)
                    if len(predictions.shape) > 1:
                        predicted_classes = torch.argmax(predictions, dim=1) 
                        accuracy = (predicted_classes == test_labels).float().mean().item()
                    else:
                        accuracy = 1.0 - torch.abs(predictions - test_labels).mean().item()
                    
                accuracies.append(accuracy)
            
            # Statistical analysis
            mean_accuracy = np.mean(accuracies)
            std_accuracy = np.std(accuracies)
            accuracy_drop = (baseline_accuracy - mean_accuracy) / baseline_accuracy
            
            variation_results[f"variation_{variation:.2f}"] = {
                "mean_accuracy": mean_accuracy,
                "std_accuracy": std_accuracy, 
                "accuracy_drop": accuracy_drop,
                "confidence_interval": stats.t.interval(
                    0.95, len(accuracies) - 1, 
                    loc=mean_accuracy, 
                    scale=std_accuracy / np.sqrt(len(accuracies))
                )
            }
            
            logger.info(f"Variation {variation:.1%}: accuracy drop {accuracy_drop:.1%}")
        
        return variation_results
    
    def comparative_study(
        self,
        spintronic_results: List[BenchmarkResult],
        baseline_results: List[BenchmarkResult],
        study_name: str = "comparative_study"
    ) -> Dict[str, Any]:
        """Perform comprehensive comparative analysis."""
        
        logger.info(f"Conducting comparative study: {study_name}")
        
        comparison = {}
        
        # Energy efficiency comparison
        spin_energies = [r.energy_per_mac_pj for r in spintronic_results]
        baseline_energies = [r.energy_per_mac_pj for r in baseline_results]
        
        energy_improvement = np.mean(baseline_energies) / np.mean(spin_energies)
        comparison["energy_improvement_factor"] = energy_improvement
        
        # Latency comparison  
        spin_latencies = [r.latency_ms for r in spintronic_results]
        baseline_latencies = [r.latency_ms for r in baseline_results]
        
        latency_ratio = np.mean(spin_latencies) / np.mean(baseline_latencies)
        comparison["latency_ratio"] = latency_ratio
        
        # Accuracy comparison
        spin_accuracies = [r.accuracy for r in spintronic_results]
        baseline_accuracies = [r.accuracy for r in baseline_results]
        
        accuracy_diff = np.mean(spin_accuracies) - np.mean(baseline_accuracies)
        comparison["accuracy_difference"] = accuracy_diff
        
        # Statistical significance testing
        energy_pvalue = stats.mannwhitneyu(spin_energies, baseline_energies).pvalue
        latency_pvalue = stats.mannwhitneyu(spin_latencies, baseline_latencies).pvalue
        accuracy_pvalue = stats.mannwhitneyu(spin_accuracies, baseline_accuracies).pvalue
        
        comparison["statistical_significance"] = {
            "energy_pvalue": energy_pvalue,
            "latency_pvalue": latency_pvalue,
            "accuracy_pvalue": accuracy_pvalue,
            "significant_energy": energy_pvalue < 0.05,
            "significant_latency": latency_pvalue < 0.05,
            "significant_accuracy": accuracy_pvalue < 0.05
        }
        
        # Overall figure of merit comparison
        spin_fom = [r.figure_of_merit() for r in spintronic_results]
        baseline_fom = [r.figure_of_merit() for r in baseline_results]
        
        fom_improvement = np.mean(spin_fom) / np.mean(baseline_fom)
        comparison["figure_of_merit_improvement"] = fom_improvement
        
        # Save results
        self._save_comparison_results(comparison, study_name)
        
        logger.info(f"Comparative study completed - Energy improvement: {energy_improvement:.1f}x")
        
        return comparison
    
    def generate_benchmark_report(self, output_file: str = "benchmark_report.json"):
        """Generate comprehensive benchmark report."""
        
        report = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "framework_version": "0.1.0",
            "total_benchmarks": len(self.results),
            "results": {}
        }
        
        for name, result in self.results.items():
            report["results"][name] = {
                "energy_per_mac_pj": result.energy_per_mac_pj,
                "latency_ms": result.latency_ms,
                "accuracy": result.accuracy,
                "area_mm2": result.area_mm2,
                "power_uw": result.power_uw,
                "throughput_ops": result.throughput_ops,
                "edap": result.energy_delay_accuracy_product(),
                "figure_of_merit": result.figure_of_merit()
            }
        
        # Statistical summary
        if self.results:
            energies = [r.energy_per_mac_pj for r in self.results.values()]
            accuracies = [r.accuracy for r in self.results.values()]
            
            report["summary"] = {
                "avg_energy_per_mac_pj": np.mean(energies),
                "std_energy_per_mac_pj": np.std(energies),
                "avg_accuracy": np.mean(accuracies),
                "std_accuracy": np.std(accuracies)
            }
        
        output_path = self.output_dir / output_file
        with open(output_path, 'w') as f:
            json.dump(report, f, indent=2)
            
        logger.info(f"Benchmark report saved to: {output_path}")
        
        return report
    
    def _estimate_energy_consumption(
        self, 
        model: nn.Module, 
        data: torch.Tensor,
        mtj_config: MTJConfig
    ) -> Tuple[float, float]:
        """Estimate energy consumption for model inference."""
        
        total_macs = 0
        total_energy = 0.0
        
        # Analyze model architecture
        for name, module in model.named_modules():
            if isinstance(module, nn.Linear):
                macs = module.in_features * module.out_features * data.shape[0]
                total_macs += macs
                
                # Energy per MAC for spintronic implementation
                switching_energy = 0.5 * mtj_config.cell_capacitance() * (mtj_config.switching_voltage ** 2)
                read_energy = (mtj_config.switching_voltage ** 2) / mtj_config.resistance_avg() * 1e-12  # 1ps read
                
                mac_energy = switching_energy + read_energy  # Joules
                total_energy += macs * mac_energy
        
        energy_per_mac_pj = (total_energy / total_macs) * 1e12  # Convert to picojoules
        
        return total_energy, energy_per_mac_pj
    
    def _estimate_area(self, model: nn.Module, mtj_config: MTJConfig) -> float:
        """Estimate silicon area for spintronic implementation."""
        
        total_area = 0.0
        
        for name, module in model.named_modules():
            if isinstance(module, nn.Linear):
                # Crossbar area estimation
                rows, cols = module.out_features, module.in_features
                crossbar_area = rows * cols * mtj_config.cell_area  # mÂ²
                
                # Peripheral circuitry overhead (approximately 3x)
                total_module_area = crossbar_area * 3.0
                total_area += total_module_area
        
        return total_area * 1e6  # Convert to mmÂ²
    
    def _get_baseline_accuracy(
        self, 
        model: nn.Module, 
        test_data: torch.Tensor, 
        test_labels: torch.Tensor
    ) -> float:
        """Get baseline accuracy without variations."""
        
        model.eval()
        with torch.no_grad():
            predictions = model(test_data)
            if len(predictions.shape) > 1:
                predicted_classes = torch.argmax(predictions, dim=1)
                accuracy = (predicted_classes == test_labels).float().mean().item()
            else:
                accuracy = 1.0 - torch.abs(predictions - test_labels).mean().item()
        
        return accuracy
    
    def _inject_device_variations(self, model: nn.Module, variation_std: float) -> nn.Module:
        """Inject device variations into model weights."""
        
        varied_model = type(model)(**{k: v for k, v in model.__dict__.items() if not k.startswith('_')})
        varied_model.load_state_dict(model.state_dict())
        
        with torch.no_grad():
            for param in varied_model.parameters():
                variation = torch.normal(0, variation_std, size=param.shape)
                param.data = param.data * (1.0 + variation)
        
        return varied_model
    
    def _save_comparison_results(self, comparison: Dict[str, Any], study_name: str):
        """Save comparative study results."""
        
        output_path = self.output_dir / f"{study_name}_comparison.json"
        with open(output_path, 'w') as f:
            # Convert numpy types for JSON serialization
            serializable_comparison = self._make_json_serializable(comparison)
            json.dump(serializable_comparison, f, indent=2)
        
        logger.info(f"Comparison results saved to: {output_path}")
    
    def _make_json_serializable(self, obj):
        """Convert numpy types to JSON-serializable types."""
        
        if isinstance(obj, dict):
            return {k: self._make_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_json_serializable(v) for v in obj]
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return obj


class ComprehensiveComparison:
    """Advanced comparison framework for academic publications."""
    
    def __init__(self, output_dir: str = "research_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def spintronic_vs_cmos_study(
        self,
        spintronic_results: List[BenchmarkResult],
        cmos_results: List[BenchmarkResult]
    ) -> Dict[str, Any]:
        """Comprehensive spintronic vs CMOS comparison study."""
        
        logger.info("Conducting spintronic vs CMOS comprehensive study")
        
        study_results = {
            "methodology": "Controlled comparison study with statistical validation",
            "sample_sizes": {
                "spintronic": len(spintronic_results),
                "cmos": len(cmos_results)
            }
        }
        
        # Energy efficiency analysis
        spin_energies = [r.energy_per_mac_pj for r in spintronic_results]
        cmos_energies = [r.energy_per_mac_pj for r in cmos_results]
        
        energy_analysis = self._statistical_comparison(
            spin_energies, cmos_energies, "Energy per MAC (pJ)"
        )
        study_results["energy_analysis"] = energy_analysis
        
        # Performance analysis  
        spin_latencies = [r.latency_ms for r in spintronic_results]
        cmos_latencies = [r.latency_ms for r in cmos_results]
        
        performance_analysis = self._statistical_comparison(
            spin_latencies, cmos_latencies, "Inference Latency (ms)"
        )
        study_results["performance_analysis"] = performance_analysis
        
        # Accuracy analysis
        spin_accuracies = [r.accuracy for r in spintronic_results] 
        cmos_accuracies = [r.accuracy for r in cmos_results]
        
        accuracy_analysis = self._statistical_comparison(
            spin_accuracies, cmos_accuracies, "Model Accuracy"
        )
        study_results["accuracy_analysis"] = accuracy_analysis
        
        # Power-performance analysis
        spin_power = [r.power_uw for r in spintronic_results if r.power_uw is not None]
        cmos_power = [r.power_uw for r in cmos_results if r.power_uw is not None]
        
        if spin_power and cmos_power:
            power_analysis = self._statistical_comparison(
                spin_power, cmos_power, "Power Consumption (Î¼W)"
            )
            study_results["power_analysis"] = power_analysis
        
        # Generate visualizations
        self._generate_comparison_plots(study_results, "spintronic_vs_cmos")
        
        # Save comprehensive results
        output_file = self.output_dir / "comprehensive_comparison_study.json"
        with open(output_file, 'w') as f:
            json.dump(study_results, f, indent=2, cls=NumpyEncoder)
        
        return study_results
    
    def _statistical_comparison(
        self, 
        group1: List[float], 
        group2: List[float],
        metric_name: str
    ) -> Dict[str, Any]:
        """Perform statistical comparison between two groups."""
        
        group1_array = np.array(group1)
        group2_array = np.array(group2)
        
        # Descriptive statistics
        stats_dict = {
            "metric": metric_name,
            "group1": {
                "mean": float(np.mean(group1_array)),
                "std": float(np.std(group1_array)), 
                "median": float(np.median(group1_array)),
                "min": float(np.min(group1_array)),
                "max": float(np.max(group1_array)),
                "n": len(group1)
            },
            "group2": {
                "mean": float(np.mean(group2_array)),
                "std": float(np.std(group2_array)),
                "median": float(np.median(group2_array)), 
                "min": float(np.min(group2_array)),
                "max": float(np.max(group2_array)),
                "n": len(group2)
            }
        }
        
        # Effect size (Cohen's d)
        pooled_std = np.sqrt(((len(group1) - 1) * np.var(group1_array) + 
                             (len(group2) - 1) * np.var(group2_array)) / 
                            (len(group1) + len(group2) - 2))
        cohens_d = (np.mean(group1_array) - np.mean(group2_array)) / pooled_std
        stats_dict["cohens_d"] = float(cohens_d)
        
        # Statistical tests
        # Mann-Whitney U test (non-parametric)
        statistic, p_value = stats.mannwhitneyu(group1_array, group2_array)
        stats_dict["mann_whitney_u"] = {
            "statistic": float(statistic),
            "p_value": float(p_value),
            "significant": p_value < 0.05
        }
        
        # T-test (parametric, assuming normality)
        t_stat, t_p_value = stats.ttest_ind(group1_array, group2_array)
        stats_dict["t_test"] = {
            "statistic": float(t_stat),
            "p_value": float(t_p_value),
            "significant": t_p_value < 0.05
        }
        
        # Confidence intervals
        group1_ci = stats.t.interval(
            0.95, len(group1) - 1,
            loc=np.mean(group1_array),
            scale=stats.sem(group1_array)
        )
        group2_ci = stats.t.interval(
            0.95, len(group2) - 1,
            loc=np.mean(group2_array), 
            scale=stats.sem(group2_array)
        )
        
        stats_dict["confidence_intervals"] = {
            "group1_95ci": [float(group1_ci[0]), float(group1_ci[1])],
            "group2_95ci": [float(group2_ci[0]), float(group2_ci[1])]
        }
        
        return stats_dict
    
    def _generate_comparison_plots(self, study_results: Dict[str, Any], study_name: str):
        """Generate publication-quality comparison plots."""
        
        # Create figure with subplots
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        fig.suptitle(f"Comprehensive Comparison Study: {study_name}", fontsize=16)
        
        # Energy comparison
        if "energy_analysis" in study_results:
            self._plot_comparison(
                axes[0, 0],
                study_results["energy_analysis"],
                "Energy per MAC (pJ)",
                log_scale=True
            )
        
        # Performance comparison  
        if "performance_analysis" in study_results:
            self._plot_comparison(
                axes[0, 1],
                study_results["performance_analysis"], 
                "Inference Latency (ms)"
            )
        
        # Accuracy comparison
        if "accuracy_analysis" in study_results:
            self._plot_comparison(
                axes[1, 0],
                study_results["accuracy_analysis"],
                "Model Accuracy"
            )
        
        # Power comparison
        if "power_analysis" in study_results:
            self._plot_comparison(
                axes[1, 1],
                study_results["power_analysis"],
                "Power Consumption (Î¼W)",
                log_scale=True
            )
        
        plt.tight_layout()
        plot_file = self.output_dir / f"{study_name}_comparison.png"
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"Comparison plots saved to: {plot_file}")
    
    def _plot_comparison(self, ax, analysis_data: Dict, ylabel: str, log_scale: bool = False):
        """Create individual comparison plot."""
        
        group1_data = analysis_data["group1"]
        group2_data = analysis_data["group2"]
        
        # Box plot
        data = [
            [group1_data["mean"]] * group1_data["n"],
            [group2_data["mean"]] * group2_data["n"]
        ]
        
        box_plot = ax.boxplot(data, labels=['Spintronic', 'CMOS'], patch_artist=True)
        box_plot['boxes'][0].set_facecolor('lightblue')
        box_plot['boxes'][1].set_facecolor('lightcoral')
        
        # Add statistical significance indication
        if analysis_data["mann_whitney_u"]["significant"]:
            ax.text(0.5, 0.95, f"p < 0.05*", transform=ax.transAxes, ha='center')
        
        ax.set_ylabel(ylabel)
        if log_scale:
            ax.set_yscale('log')
        
        # Add Cohen's d
        ax.text(0.02, 0.02, f"Effect size (d): {analysis_data['cohens_d']:.2f}", 
                transform=ax.transAxes, fontsize=8)


class NumpyEncoder(json.JSONEncoder):
    """JSON encoder for numpy data types."""
    
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return super(NumpyEncoder, self).default(obj)